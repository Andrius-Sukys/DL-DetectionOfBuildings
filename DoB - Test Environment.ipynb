{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Detection of Buildings (Test Environment)**\n",
        "\n",
        "#### **Team Bag – Andrius Šukys, Benas Skripkiūnas, Greta Virpšaitė**\n",
        "\n",
        "This is the test environment for project Detection of Buildings.\n",
        "\n",
        "In order to test a model, download a selected model from Google Drive, upload it to path `../models/` of your project and run the REST API interface.\n",
        "\n",
        "**Do not forget to upload the included file `segmentator.html` to root folder of the project, otherwise the test environment will not work!**\n",
        "\n",
        "The environment was made by the creators of the project to test it themselves. For this reason, some values are hard-coded and might need to be adjusted individually:\n",
        "\n",
        "* Mean/Std in Block 2 (in section Transformations).\n",
        "* Directory/Name of the model and Threshold in Block 6 (in section API).\n",
        "\n",
        "#### **The source codes and the final report of the project are all available on [GitHub](https://github.com/Andrius-Sukys/DL-DetectionOfBuildings).**\n",
        "\n",
        "#### **The datasets, predictions and output models due to their big size (GitHub does not allow to save them there) of the project are all available on [Google Drive](https://drive.google.com/drive/folders/1jMvX0dYo60M-s_ZaNAo_Fe_TC1f5Tut7?usp=sharing).**"
      ],
      "metadata": {
        "id": "NYW23QeyziSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup of needed Modules and Libraries\n"
      ],
      "metadata": {
        "id": "5HsrYE9t3aFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations.pytorch import ToTensorV2\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as alb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as F"
      ],
      "metadata": {
        "id": "EBYIs4F-TcYx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations"
      ],
      "metadata": {
        "id": "f5r3MjCR3dj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: DO NOT FORGET TO UPDATE THE MEAN AND STD!**"
      ],
      "metadata": {
        "id": "QQak-V8T4Fe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0, 0, 0]\n",
        "std = [1, 1, 1]\n",
        "# mean = [0.485, 0.456, 0.406]\n",
        "# std = [0.229, 0.224, 0.225]\n",
        "\n",
        "validation_transform = alb.Compose([alb.Resize(height = 224, width = 224),\n",
        "                                    alb.Normalize(mean = mean, std = std, max_pixel_value = 255.0),\n",
        "                                    ToTensorV2()])"
      ],
      "metadata": {
        "id": "a9cGoBBmUNmp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model based on UNet Architecture"
      ],
      "metadata": {
        "id": "Iag_X_Hc3hzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generally used to deepen the neural network for better performance\n",
        "class DoubleConvolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    # Double convolution as a sequence of operations applied to data\n",
        "    self.double_conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1), # 2D convolution\n",
        "                                     nn.BatchNorm2d(out_channels), # Batch normalization\n",
        "                                     nn.ReLU(inplace = True), # The original input is modified instead of creating a new tensor\n",
        "                                     nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1), # 2D convolution\n",
        "                                     nn.BatchNorm2d(out_channels), # Batch normalization\n",
        "                                     nn.ReLU(inplace = True)) # The original input is modified instead of creating a new tensor\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv(x)\n",
        "\n",
        "# Used to reduce the dimensions of input feature maps and increase receptive field\n",
        "class DownSample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    # Performs max pooling and double convolution on reduced feature maps while increasing channels\n",
        "    self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConvolution(in_channels, out_channels))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.maxpool_conv(x)\n",
        "\n",
        "# Used to increase the dimensions of the downsampled feature maps\n",
        "class UpSample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    # Used to increase the spatial dimensions of feature maps while decreasing the number of channels\n",
        "    # The feature map grows twice each time because of kernel_size and stride but the channels get reduced\n",
        "    self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size = 2, stride = 2)\n",
        "    self.conv = DoubleConvolution(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "\n",
        "    # x2 is cropped to match the dimensions of x1\n",
        "    # shape[1] is the number of channels, shape[2] is for height, shape[3] is for width\n",
        "    x2 = F.center_crop(x2, [x1.shape[2], x1.shape[3]])\n",
        "\n",
        "    # Combines both feature maps to preserve both deep and shallow level details, concatenated horizontally\n",
        "    x1 = torch.cat([x2, x1], dim = 1)\n",
        "\n",
        "    return self.conv(x1)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(UNet, self).__init__()\n",
        "    self.inc = (DoubleConvolution(in_channels, 64))\n",
        "    self.down1 = (DownSample(64, 128))\n",
        "    self.down2 = (DownSample(128, 256))\n",
        "    self.down3 = (DownSample(256, 512))\n",
        "    self.down4 = (DownSample(512, 1024))\n",
        "    self.up1 = (UpSample(1024, 512))\n",
        "    self.up2 = (UpSample(512, 256))\n",
        "    self.up3 = (UpSample(256, 128))\n",
        "    self.up4 = (UpSample(128, 64))\n",
        "    # Used to decrease the number of channels so the kernel_size is 1\n",
        "    self.outc = nn.Conv2d(64, out_channels, kernel_size = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.inc(x)\n",
        "    x2 = self.down1(x1)\n",
        "    x3 = self.down2(x2)\n",
        "    x4 = self.down3(x3)\n",
        "    x5 = self.down4(x4)\n",
        "    x = self.up1(x5, x4)\n",
        "    x = self.up2(x, x3)\n",
        "    x = self.up3(x, x2)\n",
        "    x = self.up4(x, x1)\n",
        "    logits = self.outc(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "cm1IfdgGTbAJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization of Device\n",
        "\n",
        "Recommended to use GPU *(cuda:0)* for optimal performance.\n"
      ],
      "metadata": {
        "id": "3qgKPbDk3lef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Used device type is', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN0Nl2Doz8DY",
        "outputId": "a0d2c732-a340-44f8-d4c9-d98ec0db7080"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used device type is cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API ##"
      ],
      "metadata": {
        "id": "Rh7pkw2C3qHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "API is used for testing purposes. A user may upload an image and get the mask of buildings back.\n",
        "\n",
        "**File segmentator.html must be added to the root folder for the API interface to work!**\n",
        "\n",
        "**Likewise, a model must be provided!**"
      ],
      "metadata": {
        "id": "_3QwoH533qtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the link as the API interface."
      ],
      "metadata": {
        "id": "BLNWBI4u3wQ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2S5SiL_BTK6C",
        "outputId": "cc076fbb-e930-448e-876c-86047dde3de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://jsuzdk2cf2l-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"
          ]
        }
      ],
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting the threshold and loading the uploaded model.\n",
        "\n",
        "**NOTE: DO NOT FORGET TO UPDATE THE MODEL_PATH!**\n",
        "\n",
        "**NOTE: DO NOT FORGET TO UPDATE THE THRESHOLD!**"
      ],
      "metadata": {
        "id": "tn4KZbfv3xv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(in_channels = 3, out_channels = 1).to(device)\n",
        "model_path = f'../models/UNet_Threshold_0.4.pth'\n",
        "\n",
        "threshold = 0.4\n",
        "\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOdXDU1TOUs",
        "outputId": "536fab7b-201f-48e8-de90-c71bf28eb5ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Flask to handle user requests."
      ],
      "metadata": {
        "id": "H3bozcbs4A6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request\n",
        "import torchvision.transforms as transforms\n",
        "import io\n",
        "import base64\n",
        "\n",
        "app = Flask(__name__, template_folder='../')\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    original_image_base64 = None\n",
        "    mask_base64 = None\n",
        "\n",
        "    if request.method == 'POST':\n",
        "\n",
        "        file = request.files['file']\n",
        "        original_image = np.array(Image.open(file).convert(\"RGB\"))\n",
        "\n",
        "        transformed_image = validation_transform(image = original_image)['image']\n",
        "\n",
        "        predicted_mask = model(torch.unsqueeze(transformed_image, 0).to(device))\n",
        "        predicted_mask = torch.sigmoid(predicted_mask)\n",
        "\n",
        "        # Change the threshold to a desired one!\n",
        "        predicted_mask = (predicted_mask > threshold).float().cpu().squeeze().numpy()\n",
        "\n",
        "        mask_pil = transforms.ToPILImage()(predicted_mask)\n",
        "\n",
        "        original_image_buffered = io.BytesIO()\n",
        "        mask_buffered = io.BytesIO()\n",
        "\n",
        "        Image.fromarray(original_image).save(original_image_buffered, format = \"PNG\")\n",
        "        mask_pil.save(mask_buffered, format = \"PNG\")\n",
        "\n",
        "        original_image_base64 = base64.b64encode(original_image_buffered.getvalue()).decode('utf-8')\n",
        "        mask_base64 = base64.b64encode(mask_buffered.getvalue()).decode('utf-8')\n",
        "\n",
        "    # Don't forget to upload segmentator.html to main folder!\n",
        "    return render_template('segmentator.html', original_image = original_image_base64, mask = mask_base64)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.run()"
      ],
      "metadata": {
        "id": "GBUvt8-ZTPKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9777782c-4bbf-4b1a-fb75-045efdd62cdf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:43:30] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:43:31] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:43:38] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:43:39] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:43:49] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:43:49] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:46:13] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:46:15] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:46:50] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2024 13:46:52] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ]
    }
  ]
}